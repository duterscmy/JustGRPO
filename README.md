<div align="center">

# JustGRPO

**The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models**

<p align="center">
    <a href="https://nzl-thu.github.io/">Zanlin Ni<sup>1</sup></a> &emsp;
    <a href="https://scholar.google.com/citations?user=Xgt7njgAAAAJ&hl=zh-CN">Shenzhi Wang<sup>1</sup></a> &emsp;
    <a href="https://yueyang130.github.io/">Yang Yue<sup>1</sup></a> &emsp;
    <a href="https://scholar.google.com/citations?user=e-FRHr4AAAAJ&hl=zh-TW">Tianyu Yu<sup>2</sup></a> &emsp;
    <a href="https://brawny-college-5b2.notion.site/Weilin-Zhao-11d20b7deb8280388213d5f5ed072992">Weilin Zhao<sup>2</sup></a> &emsp;
    <a href="https://dblp.uni-trier.de/pid/402/2123.html">Yeguo Hua<sup>3</sup></a> &emsp;
</p>
<p align="center">
    Tianyi Chen<sup>3</sup> &emsp;
    Jun Song<sup>4</sup> &emsp;
    Cheng Yu<sup>4</sup> &emsp;
    Bo Zheng<sup>4</sup> &emsp;
    <a href="https://gaohuang-net.github.io/">Gao Huang<sup>1‚úâ</sup></a>
</p>

<p align="center">
    <sup>1</sup>LeapLab, Tsinghua University &emsp;
    <sup>2</sup>NLPLab, Tsinghua University &emsp;
    <sup>3</sup>Tsinghua University &emsp;
    <sup>4</sup>Alibaba Group
</p>

[![Project](https://img.shields.io/badge/üåê%20Project-Page-green)](https://nzl-thu.github.io/the-flexibility-trap/)
[![arXiv](https://img.shields.io/badge/arXiv-2601.15165-b31b1b.svg)](https://arxiv.org/abs/2601.15165)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Model](https://img.shields.io/badge/ü§ó%20Model-JustGRPO-yellow)](https://huggingface.co/nzl-thu/LLaDA-Instruct-JustGRPO)

*No combinatorial trajectories. No ELBO approximations. No diffusion-specific adaptations.*

**Just GRPO.**

</div>


## üì¢ News

- **[2026.01]** üìÑ Paper available on [arXiv](https://arxiv.org/abs/2601.15165)!
- **[2026.01]** üéâ Training code, evaluation scripts, and [model checkpoint](https://huggingface.co/nzl-thu/LLaDA-Instruct-JustGRPO) on GSM8K released!

## üìã TODO

- [ ] Add support for MATH-500
- [ ] Add support for coding tasks (HumanEval, MBPP)


## Why JustGRPO?

Diffusion LLMs (dLLMs) can generate tokens in **arbitrary order**, which theoretically offers more flexibility than standard left-to-right generation. But does this flexibility actually unlocks unique reasoning capabilities inaccessible to standard AR models?

<div align="center">
  <img src="assets/mechanism_to_passk.png" width="90%" alt="Mechanism to Pass@k"/>
</div>

**We found the opposite.** Arbitrary-order generation allows models to *bypass* high-uncertainty tokens (e.g., "Therefore", "Since") ‚Äî the very tokens that create branching points in reasoning. This premature bypass collapses the solution space, leading to *lower* reasoning potential (Pass@k).

**Our solution is simple:** Since AR order preserves better reasoning potential, we just train dLLMs with standard GRPO in AR mode. No bells and whistles.


## Results

JustGRPO achieves state-of-the-art performance across reasoning and coding benchmarks:

<div align="center">
  <img src="assets/acc_compare.png" width="90%" alt="Accuracy Comparison"/>
</div>


## Simplicity

Existing RL methods for dLLMs often require handling the complexity of arbitrary-order generation:

| Challenge | Description |
|:---|:---|
| Combinatorial trajectories | Optimizing over factorial-sized denoising paths |
| Intractable likelihoods | ELBO-based surrogates instead of true objectives |
| Sampler-learner mismatch | Confidence-based samplers vs. original diffusion prior |

- **JustGRPO sidesteps all of this** by treating dLLMs as autoregressive models during RL training. The result? Standard GRPO, directly applicable, with exact likelihood computation.
- **The core logic of JustGRPO (`grpo.py`) fits in ~60 lines**: rollout sampling and log-probability loss computation. That's it.

> üí° The model still retains **parallel decoding** at inference time ‚Äî we only use AR order during training. See our paper for more details.



## Installation

JustGRPO is designed to be lightweight and dependency-minimal.

```bash
git clone https://github.com/LeapLabTHU/JustGRPO.git
cd JustGRPO
pip install -r requirements.txt
```

**Dependencies:**
- `accelerate`
- `transformers`
- `datasets`
- Standard evaluation utilities (`sympy`, `latex2sympy2`, etc.)


## Usage

We provide training and evaluation code on **GSM8K**.
The RL-trained model is available at [Huggingface](https://huggingface.co/nzl-thu/LLaDA-Instruct-JustGRPO).

### Training

```bash
accelerate launch --num_processes 8 --main_process_ip localhost --config_file configs/fsdp.yaml train.py \
  --run_dir ./checkpoints \
  --grad_accum 8
```
- Note: The global batch size = num_gpus * grad_accum, keep it equal to 64.

### Evaluation

```bash
torchrun --standalone --nproc-per-node=8 eval.py \
  --ckpt_path /path/to/ckpt \
  --steps 256 \
  --gen_length 256 \
  --block_length 32
```


## Citation

If you find this work useful, please cite:

```bibtex
@article{ni2026flexibility,
  title={The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models},
  author={Ni, Zanlin and Wang, Shenzhi and Yue, Yang and Yu, Tianyu and Zhao, Weilin and Hua, Yeguo and Chen, Tianyi and Song, Jun and Yu, Cheng and Zheng, Bo and Huang, Gao},
  journal={arXiv preprint arXiv:2601.15165},
  year={2026}
}
```

## Acknowledgments

This project builds upon the following excellent works:

- [LLaDA](https://github.com/ML-GSAI/LLaDA)
- [d1](https://github.com/dllm-reasoning/d1)
- [LLaDOU](https://github.com/maple-research-lab/LLaDOU)

We thank the authors for their open-source contributions to the community.
